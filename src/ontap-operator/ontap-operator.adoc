:toc: left
:toclevels: 3
:imagesdir: ../images
:source-highlighter: highlightjs
= ONTAP Operator


== Overview

The purpose of this blog post is to build a proof of concept Kubernetes Ansible based operator using the Red Hat operator framework. Aside from a bit of reading and some basic testing, I have very limited previous experience with the SDK so this page will be a step by step walkthrough including any mistakes I make along the way.

Operators are usually used to manage K8s resources, so the goal is to see if I can create one to manage an external NetApp ONTAP storage in a development environment. The motivation is to better understand how the SDK works, how easy (or difficult as the case may be) it is to use and see if the operator could ultimately replace Red Hat Ansible Tower/AWX. I'm not sure I'll have all the answers by the end of this blog post but let's see how we go.

If you are unfamiliar with Kubernetes operators the links below do a great job at describing what they are and also how Ansible (via the SDK) can be leveraged to build one:

* https://kubernetes.io/docs/concepts/extend-kubernetes/operator/
* https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator
* https://www.ansible.com/blog/ansible-operator

Over at https://netapp.io/[the pub] the NetApp developers and automation specialists continue to develop top class Ansible roles and modules to manage ONTAP storage, all made available as an https://galaxy.ansible.com/netapp/ontap[Ansible galaxy collection]. Given how mature the ONTAP Ansible collection is, the theory is that using the operator-sdk with the Ansible option should result in a simple to build yet super powerful operator to manage ONTAP.

For the initial proof of concept, let's see if we can have the operator to perform the following three basic tasks:

1. Create a storage virtual machine (SVM).
2. Create a volume.
3. Create a NFS share.

We can mount the NFS share for completeness and I should then also be able to remove any resources by deleting the objects in K8s.

Alright, let's get going...

== Development Environment

First, I'll need to prepare my development environment.

=== Operator SDK installation

I use Pop!_OS on a Dell laptop at home and so start by following the https://sdk.operatorframework.io/docs/installation/["Install from GitHub release"] documentation to grab the operator-sdk binary. I follow the instructions step by step and once done validate that have the expected version of the operator-sdk:

```bash
❯ operator-sdk version
operator-sdk version: "v1.16.0", commit: "560044140c4f3d88677e4ef2872931f5bb97f255", kubernetes version: "1.21", go version: "go1.16.13", GOOS: "linux", GOARCH: "amd64"
```
Good to go there.

=== Kubernetes (Rancher Desktop)

Next up, I'll need a Kubernetes environment to test the operator on. While I do have a Rancher RKE1 cluster running in my home lab, I like the idea of using a local K8s environment. I'm a fan of Rancher and so will install Rancher Desktop by following https://docs.rancherdesktop.io/installation/#installation-via-deb-package[the instructions on the Rancher Desktops website] to use a local K8s development instance moving forward. Once installed, I launch Rancher Desktop and am prompted with a couple options:

image::ansible-ontap-operator_welcome-to-rancher-desktop.png[]

I'll stick with the defaults use containerd as my runtime and the latest version of Kubernetes available in Rancher Desktop.

The https://docs.rancherdesktop.io/faq[FAQ] says I will find my configuration file in the default location (~/.kube/config). I do have a seperate config file for my RKE1 cluster so add that file to my KUBECONFIG environment variable list:

```bash
# I use zsh with oh-my-zsh so add the below line to $HOME/.oh-my-zsh/custom/config.zsh:
export KUBECONFIG='$KUBECONFIG:$HOME/.kube/config'

# Source the changes
❯ source $HOME/.oh-my-zsh/custom/config.zsh

# Validate I can see and use the new context
❯ kubectl config get-contexts
CURRENT   NAME              CLUSTER           AUTHINFO          NAMESPACE
          rancher-desktop   rancher-desktop   rancher-desktop   
*         rke1              rke1              rke1

# Swap to the new context
❯ kubectl config use-context rancher-desktop
Switched to context "rancher-desktop".

# Test the API
❯ kubectl get nodes         
NAME                   STATUS   ROLES                  AGE   VERSION
lima-rancher-desktop   Ready    control-plane,master   31m   v1.23.2+k3s1
```

Looking good!

=== ONTAP
Finally, we'll need the details of the ONTAP system. For this POC I'm using the NetApp ONTAP simulator to simulate a single node ONTAP system running in VMware Workstation. I'll keep the setup of the ONTAP simulator out of scope in this blog to focus on the operator bits but we can note down the following basic out of the box configuration:

|=== 
|Configuration Item |Value

|ONTAP Version
|9.9.1

|Cluster Name
|ontap-sim

|Node Name
|ontap-sim-01

|Cluster IP Address
|192.168.2.201

|Node IP address
|192.168.2.200

|Data Aggregate
|ontap_sim_01_FC_1

|Username
|admin

|Password
|Pa$$w0rd

|===

== Building The Operator

With the development environment prepared we should be ready to rock and roll.

=== Scaffolding With The SDK

Let's create a new folder to store the operator code then initialize the project using the operatork-sdk binary by following along with the https://sdk.operatorframework.io/docs/building-operators/ansible/tutorial/[Ansible Operator Tutorial] documentation. 

I'm going to give this operator the very original name of "ontap-operator". I'll also use my domain "vaughanross.io" but this could be anything for the purposes of the POC.

```bash
❯ mkdir ontap-operator 
❯ cd ontap-operator
❯ operator-sdk init --plugins ansible --domain vaughanross.io
Writing kustomize manifests for you to edit...
Next: define a resource with:
$ operator-sdk create api
```

That was easy. Using VSCode or running 'tree' in the terminal I can see the SDK has generated 16 new directories and 45 files. 

Next, we'll generate custom resource definitions (CRDs) to extend the Kubernetes API. Considering the scope of the POC is to manage SVM's, Volumes and Shares, it's reasonable to create a CRD for each type and group them using the name 'ontap'.

```bash
❯ operator-sdk create api --group ontap --version v1alpha1 --kind StorageVirtualMachine --generate-role
Writing kustomize manifests for you to edit...
❯ operator-sdk create api --group ontap --version v1alpha1 --kind Volume --generate-role
Writing kustomize manifests for you to edit...
❯ operator-sdk create api --group ontap --version v1alpha1 --kind Share --generate-role
Writing kustomize manifests for you to edit...
```

Using tree I can see the SDK has created a role with the usual Ansible directory structure for each type under the roles folder. Very cool.

```bash
❯ tree roles -d    
roles
├── share
│   ├── defaults
│   ├── files
│   ├── handlers
│   ├── meta
│   ├── tasks
│   ├── templates
│   └── vars
├── storagevirtualmachine
│   ├── defaults
│   ├── files
│   ├── handlers
│   ├── meta
│   ├── tasks
│   ├── templates
│   └── vars
└── volume
    ├── defaults
    ├── files
    ├── handlers
    ├── meta
    ├── tasks
    ├── templates
    └── vars
```

Next, let's review some of the files in the root of the project folder and update them if required.

```bash
❯ cat Dockerfile    
FROM quay.io/operator-framework/ansible-operator:v1.16.0

COPY requirements.yml ${HOME}/requirements.yml
RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \
 && chmod -R ug+rwx ${HOME}/.ansible

COPY watches.yaml ${HOME}/watches.yaml
COPY roles/ ${HOME}/roles/
COPY playbooks/ ${HOME}/playbooks/
```

We can see the Dockerfile will install any collections found in the requirements.yml file. It also copies the watches.yaml file along with the roles and playbooks folder in to the container. Fair enough! We know we'll need the ontap collection from Ansible galaxy, so let's add it to the requirements.yml file.

```yaml
---
collections:
  - name: community.kubernetes
    version: "1.2.1"
  - name: operator_sdk.util
    version: "0.3.1"
  - name: kubernetes.core
    version: "2.2.0"
#  My addition is below this line.
  - name: netapp.ontap
    version: "21.15.1"
```

Finally, let's review the watches.yaml file.

```bash
cat watches.yaml
---
# Use the 'create api' subcommand to add watches to this file.
- version: v1alpha1
  group: ontap.vaughanross.io
  kind: StorageVirtualMachine
  role: storagevirtualmachine
- version: v1alpha1
  group: ontap.vaughanross.io
  kind: Volume
  role: volume
- version: v1alpha1
  group: ontap.vaughanross.io
  kind: Share
  role: share
#+kubebuilder:scaffold:watch
```
We note that this file tells the operator to watch each CRD created earlier and run the corresponding role when changes are detected. We will most likely need to come back and fine tune this later.

At this stage, I'm confident we have the scaffolding in place and although it's not functional, I'd like to test the build, test and deployment pipeline. Fortunately, the SDK generated a Makefile with convenient preconfigured targets to use in the pipeline.

=== Container Image Creation

I've created a public repository "vaughanross/ontap-operator" on DockerHub to store the operator image, and so make the following adjustments to the Makefile.

```git
-IMAGE_TAG_BASE ?= vaughanross.io/ontap-operator
+IMAGE_TAG_BASE ?= vaughanross/ontap-operator

-IMG ?= controller:latest
+IMG ?= $(IMAGE_TAG_BASE):$(VERSION)

```

I've run docker auth to authenticate to the repository and so should now be able to build and push the container image using the Makefile:

```bash
❯ make docker-build docker-push VERSION=0.0.1
docker build -t vaughanross/ontap-operator:0.0.1 .
Sending build context to Docker daemon  34.18MB
Step 1/6 : FROM quay.io/operator-framework/ansible-operator:v1.16.0
v1.16.0: Pulling from operator-framework/ansible-operator
26f1167feaf7: Pull complete 
adffa6963146: Pull complete 
34392db44a7a: Pull complete 
f87d97c96420: Pull complete 
45ceea710a34: Pull complete 
d80aabd8d47a: Pull complete 
ba4214f4154e: Pull complete 
d349a89ada27: Pull complete 
Digest: sha256:df76f066bc267be1d73a701b6791abcb4646b3d31c70ba31b2fca778a8bc99de
Status: Downloaded newer image for quay.io/operator-framework/ansible-operator:v1.16.0
 ---> 352cac8432b5
Step 2/6 : COPY requirements.yml ${HOME}/requirements.yml
 ---> bde47aec3a17
Step 3/6 : RUN ansible-galaxy collection install -r ${HOME}/requirements.yml  && chmod -R ug+rwx ${HOME}/.ansible
 ---> Running in 2e18b6ad0a60
Process install dependency map
Starting collection install process
Installing 'community.kubernetes:1.2.1' to '/opt/ansible/.ansible/collections/ansible_collections/community/kubernetes'
Installing 'operator_sdk.util:0.3.1' to '/opt/ansible/.ansible/collections/ansible_collections/operator_sdk/util'
Installing 'kubernetes.core:2.2.0' to '/opt/ansible/.ansible/collections/ansible_collections/kubernetes/core'
Installing 'netapp.ontap:21.15.1' to '/opt/ansible/.ansible/collections/ansible_collections/netapp/ontap'
Removing intermediate container 2e18b6ad0a60
 ---> 30c9915c8081
Step 4/6 : COPY watches.yaml ${HOME}/watches.yaml
 ---> a4b20b3df0af
Step 5/6 : COPY roles/ ${HOME}/roles/
 ---> 9a26e1daefd0
Step 6/6 : COPY playbooks/ ${HOME}/playbooks/
 ---> e03cac920462
Successfully built e03cac920462
Successfully tagged vaughanross/ontap-operator:0.0.1
docker push vaughanross/ontap-operator:0.0.1
The push refers to repository [docker.io/vaughanross/ontap-operator]
7360d40299aa: Pushed 
f3eaeb4ecc8b: Pushed 
e0795cc39e1e: Pushed 
fe37890326dd: Pushed 
f85b884a7c0b: Pushed 
a6ee3241e4b6: Pushed 
40a9bab6edf7: Pushed 
67235be480de: Pushed 
c1ec8952b903: Pushed 
7bb5fcd3789d: Pushed 
e3313ed16678: Pushed 
3ba8c926eef9: Pushed 
352ba846236b: Pushed 
0.0.1: digest: sha256:4f1d34aa235a9630f3fbd9215fb04a3161d1444f35778311457d2843f0311de5 size: 3033
```
No errors and we can see from the build logs the controller has installed the ontap collection.

=== Operator Deployment

With the image pushed to the registry, let's try deploy the operator to Kubernetes:

```bash
❯ make deploy VERSION=0.0.1                  
cd config/manager && /home/vaughan/code/vaughanross/ontap-operator/bin/kustomize edit set image controller=vaughanross/ontap-operator:0.0.1
/home/vaughan/code/vaughanross/ontap-operator/bin/kustomize build config/default | kubectl apply -f -
namespace/ontap-operator-system created
customresourcedefinition.apiextensions.k8s.io/shares.ontap.vaughanross.io created
customresourcedefinition.apiextensions.k8s.io/storagevirtualmachines.ontap.vaughanross.io created
customresourcedefinition.apiextensions.k8s.io/volumes.ontap.vaughanross.io created
serviceaccount/ontap-operator-controller-manager created
role.rbac.authorization.k8s.io/ontap-operator-leader-election-role created
clusterrole.rbac.authorization.k8s.io/ontap-operator-manager-role created
clusterrole.rbac.authorization.k8s.io/ontap-operator-metrics-reader created
clusterrole.rbac.authorization.k8s.io/ontap-operator-proxy-role created
rolebinding.rbac.authorization.k8s.io/ontap-operator-leader-election-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/ontap-operator-manager-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/ontap-operator-proxy-rolebinding created
configmap/ontap-operator-manager-config created
service/ontap-operator-controller-manager-metrics-service created
deployment.apps/ontap-operator-controller-manager created
```
That's a fair bit. Reviewing the list we see the Makefile deploy target has kustomized and deployed all resources, including;

* The namespace "ontap-operator-system"
* The custom resource definitions
* The RBAC serviceaccounts, roles, rolebindings
* The operator deployment along with a configmap

Let's ensure the deployment is running and verify the image versions:

```bash
❯ kubectl get deployment -n ontap-operator-system
NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
ontap-operator-controller-manager   1/1     1            1           5m44s

❯ kubectl get pods --namespace ontap-operator-system --output=custom-columns="NAME:.metadata.name,IMAGE:.spec.containers[*].image" 
NAME                                                IMAGE
ontap-operator-controller-manager-c4f457795-454vb   gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0,vaughanross/ontap-operator:0.0.1
```

Great. Note that from here on out as I make changes to the operator I'll be incrementing the version using the above workflow but not pasting the output in this blog.

=== Role Configuration

This is where the rubber hits the road. We've successfully built and deployed our operator, created the CRD's and have the controller watching them for changes. We now need to configure the roles to tell the operator what to do.

==== Storage Virtual Machine

We'll need an SVM before we can create a volume and share, so start by modifying the Storage Virtual Machine role using the https://docs.ansible.com/ansible/latest/collections/netapp/ontap/na_ontap_svm_module.html[netapp.ontap.na_ontap_svm module] documentation as a guide. I'll add the below content to the defaults and tasks main.yml files to comply with the documentaiton.

```yaml
---
# defaults file for StorageVirtualMachine
validate_certs: no
```
```yaml
---
# tasks file for StorageVirtualMachine
- name: Create SVM
  netapp.ontap.na_ontap_svm:
    state: "{{ state }}"
    name: "{{ svm_name }}"
    services:
      cifs:
        allowed: "{{ cifs_allowed }}"
      fcp:
        allowed: "{{ fcp_allowed }}"
      nfs:
        allowed: "{{ nfs_allowed }}"
        enabled: "{{ nfs_enabled }}"
    hostname: "{{ netapp_hostname }}"
    username: "{{ netapp_username }}"
    password: "{{ netapp_password }}"
    https: "{{ https_enabled }}"
    validate_certs: "{{ validate_certs }}"
```
That's all there is to it. After another build, push and deploy we should be able to create an object in Kubernetes of the kind "StorageVirtualMachine". The SDK has generated a handy template for us to use under config/samples/ontap_v1alpha1_storagevirtualmachine.yaml:

```yaml
apiVersion: ontap.vaughanross.io/v1alpha1
kind: StorageVirtualMachine
metadata:
  name: storagevirtualmachine-sample
spec:
  # TODO(user): Add fields here
```
Note that each item under 'spec' becomes a variable that's passed to the operator and used in the playbook. Reviewing the na_ontap_svm_module documentation, I edit the file to create an SVM called 'nfs-svm' to serve NFS over IP address 192.168.2.205:

```yaml
apiVersion: ontap.vaughanross.io/v1alpha1
kind: StorageVirtualMachine
metadata:
  name: nfs-svm
spec:
  state: present
  svm_name: nfs-svm
  cifs_allowed: false
  fcp_allowed: false
  nfs_allowed: true
  nfs_enabled: true
  netapp_hostname: 192.168.2.201
  netapp_username: admin
  netapp_password: Pa$$w0rd
  https_enabled: true
  validate_certs: no
```

Let's give it a try:

```console
❯ kubectl apply -f ontap-operator/config/samples/ontap_v1alpha1_storagevirtualmachine.yaml
storagevirtualmachine.ontap.vaughanross.io/nfs-svm created

❯ kubectl get storagevirtualmachines.ontap.vaughanross.io 
NAME          AGE
nfs-svm   76s

❯ kubectl describe storagevirtualmachines.ontap.vaughanross.io  nfs-svm
(truncated output)
    Ansible Result:
      Changed:             1
      Completion:          2022-01-28T05:10:27.619999
      Failures:            0
      Ok:                  1
      Skipped:             0
    Last Transition Time:  2022-01-28T05:10:12Z
    Message:               Awaiting next reconciliation
    Reason:                Successful
    Status:                True
    Type:                  Running
    Last Transition Time:  2022-01-28T05:10:27Z
    Message:               Last reconciliation succeeded
    Reason:                Successful
    Status:                True
    Type:                  Successful
    Last Transition Time:  2022-01-28T05:08:59Z
    Message:               
    Reason:                
    Status:                False
    Type:                  Failure
```
That actually worked! :)

Another way to validate Ansible has run is by checking the operator logs:

```console
❯ kubectl logs -n ontap-operator-system ontap-operator-controller-manager-7cd878d9c6-df9rc
--------------------------- Ansible Task StdOut -------------------------------

TASK [storagevirtualmachine : Create SVM] **************************************
task path: /opt/ansible/roles/storagevirtualmachine/tasks/main.yml:3

-------------------------------------------------------------------------------
{"level":"info","ts":1643346627.8149536,"logger":"runner","msg":"Ansible-runner exited successfully","job":"3128614846260928290","name":"nfs-svm","namespace":"default"}

----- Ansible Task Status Event StdOut (ontap.vaughanross.io/v1alpha1, Kind=StorageVirtualMachine, nfs-svm/default) -----


PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
```
I can also see the SVM in ONTAP System Manager:

image::ansible-ontap-operator_nfs-svm.png[]

How cool is that!?

Let's see what happens when we try to remove the SVM:

```bash
❯ kubectl delete -f ./config/samples/ontap_v1alpha1_storagevirtualmachine.yaml
storagevirtualmachine.ontap.vaughanross.io "nfs-svm" deleted

❯ kubectl get storagevirtualmachines.ontap.vaughanross.io -A
No resources found
```
The custom resource is removed and yet the SVM is still present (using the ONTAP CLI to validate):

```bash
ontap-sim::> vserver show
                               Admin      Operational Root
Vserver     Type    Subtype    State      State       Volume     Aggregate
----------- ------- ---------- ---------- ----------- ---------- ----------
nfs-svm     data    default    running    running     nfs_svm_   ontap_sim_
                                                      root       01_FC_1
ontap-sim   admin   -          -          -           -          -
ontap-sim-01 
            node    -          -          -           -          -
```

The reason for this is because in our watches.yml file we instruct the operator to run the role at every change, so all that's happened behind the scenes is that the role has run again. Fortunately, we can fix this with https://sdk.operatorframework.io/docs/building-operators/ansible/reference/finalizers/[finalizers].

I'll add the below configuration to the watches.yaml file:

```yaml
- version: v1alpha1
  group: ontap.vaughanross.io
  kind: StorageVirtualMachine
  role: storagevirtualmachine
# New finalizer configuration added below
  finalizer:
    name: ontap.vaughanross.io/finalizer
    vars:
      state: absent
```
Now I'll recreate and then try to remove the SVM:

```bash
❯ kubectl apply -f ./config/samples/ontap_v1alpha1_storagevirtualmachine.yaml 
storagevirtualmachine.ontap.vaughanross.io/nfs-svm created

❯ kubectl delete -f ./config/samples/ontap_v1alpha1_storagevirtualmachine.yaml
storagevirtualmachine.ontap.vaughanross.io "nfs-svm" deleted

❯ ssh admin@192.168.2.201
ontap-sim::> vserver show
                               Admin      Operational Root
Vserver     Type    Subtype    State      State       Volume     Aggregate
----------- ------- ---------- ---------- ----------- ---------- ----------
ontap-sim   admin   -          -          -           -          -
ontap-sim-01 
            node    -          -          -           -          -
2 entries were displayed.
```
Excellent. Ansible has removed the SVM.

==== Using Kubernetes Secrets

At this point, we have an established pattern that can be applied to our Volumes and Share roles but before we go any futher I'd like ensure that the cluster password is not stored in clear text as part of the specification. 

A quick Google on the topic led me to this existing https://github.com/operator-framework/operator-sdk/issues/2510[issue 2510] in the operator-sdk repo. In summary, we can add a secret to the operator deployment.

Let's create a secret for the credentials.

```bash
❯ kubectl create secret generic ontap-credentials --namespace ontap-operator-system --from-literal netapp_username=admin --from-literal netapp_password=Pa$$w0rd
secret/ontap-credentials created
```

=== Closing Thoughts

In summary I found the SDK and Ansible plugin incredibly easy to use to build out this POC while learning a fair bit along the way. I'm very interested to see how operators will be used moving forward as Kubernetes adoption continues to increase.

Any additional work on this particular operator can be tracked here: <insert GitHub URL>